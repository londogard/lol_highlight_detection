{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from data_utils.frame_dataset import FrameDataset\n",
    "import lightning as L\n",
    "import polars as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from models.lightning_wrapper import LightningWrapper\n",
    "\n",
    "model_path = \"ckpts/mobilenetv3_large_100.miil_in21k_ft_in1k.ckpt\"\n",
    "\n",
    "model = LightningWrapper.load_from_checkpoint(model_path)\n",
    "trainer = L.Trainer(accelerator=\"cpu\")\n",
    "\n",
    "paths = list(Path(\"1863051677\").rglob(\"*.jpg\"))[5_000:6_000]\n",
    "df = pl.DataFrame(\n",
    "    {\"path\": paths, \"frame\": [int(p.stem.removeprefix(\"img\")) for p in paths]}\n",
    ")\n",
    "df = df.sort(\"frame\")\n",
    "\n",
    "ds = FrameDataset(df, model.get_transforms(is_training=False), 1, is_train=False)\n",
    "dls = DataLoader(ds, batch_size=32, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06b6637671342c388307d21eb664053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "preds_list: list[torch.Tensor] = trainer.predict(model, dataloaders=dls)  # type: ignore\n",
    "preds = torch.cat(preds_list)\n",
    "pred_class = torch.argmax(preds, dim=1)\n",
    "preds_class = np.repeat(pred_class.numpy(), ds.frames_per_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as mls\n",
    "model = mls.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "batch = next(iter(dls))\n",
    "e = shap.DeepExplainer(model, batch[:10].clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Output 0 of BackwardHookFunctionBackward is a view and is being modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by cloning the output of the custom Function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m shap_values \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39;49mshap_values(batch[:\u001b[39m3\u001b[39;49m]\u001b[39m.\u001b[39;49mclone())\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/shap/explainers/_deep/__init__.py:125\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, ranked_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, output_rank_order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, check_additivity\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m        were chosen as \"top\".\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mshap_values(X, ranked_outputs, output_rank_order, check_additivity\u001b[39m=\u001b[39;49mcheck_additivity)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/shap/explainers/_deep/deep_pytorch.py:190\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m# run attribution computation graph\u001b[39;00m\n\u001b[1;32m    189\u001b[0m feature_ind \u001b[39m=\u001b[39m model_output_ranks[j, i]\n\u001b[0;32m--> 190\u001b[0m sample_phis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(feature_ind, joint_x)\n\u001b[1;32m    191\u001b[0m \u001b[39m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterim:\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/shap/explainers/_deep/deep_pytorch.py:107\u001b[0m, in \u001b[0;36mPyTorchDeep.gradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    106\u001b[0m X \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mrequires_grad_() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs]\n\u001b[0;32m--> 107\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49mX)\n\u001b[1;32m    108\u001b[0m selected \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m outputs[:, idx]]\n\u001b[1;32m    109\u001b[0m grads \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torchvision/models/resnet.py:270\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    268\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m    269\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m--> 270\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torch/nn/functional.py:1455\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, inplace\u001b[39m=\u001b[39minplace)\n\u001b[1;32m   1454\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m-> 1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu_(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39minput\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Output 0 of BackwardHookFunctionBackward is a view and is being modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by cloning the output of the custom Function."
     ]
    }
   ],
   "source": [
    "shap_values = e.shap_values(batch[:3].clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(batch[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5131acd27edb4c90a19e9c0c48e9f72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15e3f69952047dba2d78ed65c30f881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlime\u001b[39;00m \u001b[39mimport\u001b[39;00m lime_image\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m explainer \u001b[39m=\u001b[39m lime_image\u001b[39m.\u001b[39mLimeImageExplainer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m explanation \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mexplain_instance(np\u001b[39m.\u001b[39;49marray(batch[\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                          \u001b[39mlambda\u001b[39;49;00m x: trainer\u001b[39m.\u001b[39;49mpredict(model, torch\u001b[39m.\u001b[39;49mfrom_numpy(x\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m))), \u001b[39m# classification function\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                          top_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                          hide_color\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                          batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                          num_samples\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lime/lime_image.py:198\u001b[0m, in \u001b[0;36mLimeImageExplainer.explain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    194\u001b[0m     fudged_image[:] \u001b[39m=\u001b[39m hide_color\n\u001b[1;32m    196\u001b[0m top \u001b[39m=\u001b[39m labels\n\u001b[0;32m--> 198\u001b[0m data, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_labels(image, fudged_image, segments,\n\u001b[1;32m    199\u001b[0m                                 classifier_fn, num_samples,\n\u001b[1;32m    200\u001b[0m                                 batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m    202\u001b[0m distances \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mpairwise_distances(\n\u001b[1;32m    203\u001b[0m     data,\n\u001b[1;32m    204\u001b[0m     data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m    205\u001b[0m     metric\u001b[39m=\u001b[39mdistance_metric\n\u001b[1;32m    206\u001b[0m )\u001b[39m.\u001b[39mravel()\n\u001b[1;32m    208\u001b[0m ret_exp \u001b[39m=\u001b[39m ImageExplanation(image, segments)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lime/lime_image.py:261\u001b[0m, in \u001b[0;36mLimeImageExplainer.data_labels\u001b[0;34m(self, image, fudged_image, segments, classifier_fn, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    259\u001b[0m imgs\u001b[39m.\u001b[39mappend(temp)\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(imgs) \u001b[39m==\u001b[39m batch_size:\n\u001b[0;32m--> 261\u001b[0m     preds \u001b[39m=\u001b[39m classifier_fn(np\u001b[39m.\u001b[39;49marray(imgs))\n\u001b[1;32m    262\u001b[0m     labels\u001b[39m.\u001b[39mextend(preds)\n\u001b[1;32m    263\u001b[0m     imgs \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlime\u001b[39;00m \u001b[39mimport\u001b[39;00m lime_image\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m explainer \u001b[39m=\u001b[39m lime_image\u001b[39m.\u001b[39mLimeImageExplainer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m explanation \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39mexplain_instance(np\u001b[39m.\u001b[39marray(batch[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m), \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                          \u001b[39mlambda\u001b[39;00m x: trainer\u001b[39m.\u001b[39;49mpredict(model, torch\u001b[39m.\u001b[39;49mfrom_numpy(x\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m))), \u001b[39m# classification function\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                          top_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                          hide_color\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                          batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                          num_samples\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:865\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    864\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    866\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    867\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:904\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    902\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    903\u001b[0m )\n\u001b[0;32m--> 904\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    906\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    907\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m-> 1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/loops/prediction_loop.py:122\u001b[0m, in \u001b[0;36m_PredictionLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    121\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    123\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/loops/prediction_loop.py:250\u001b[0m, in \u001b[0;36m_PredictionLoop._predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m# configure step_kwargs\u001b[39;00m\n\u001b[1;32m    245\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, \u001b[39m\"\u001b[39m\u001b[39mpredict_step\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    248\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    249\u001b[0m )\n\u001b[0;32m--> 250\u001b[0m predictions \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mpredict_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m predictions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mpredict returned None if it was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:429\u001b[0m, in \u001b[0;36mStrategy.predict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mpredict_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mpredict_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/lightning/pytorch/core/module.py:900\u001b[0m, in \u001b[0;36mLightningModule.predict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39m# For backwards compatibility\u001b[39;00m\n\u001b[1;32m    899\u001b[0m batch \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m, args[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 900\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(batch)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/hlondogard/lol_highlight_detection/models/lightning_wrapper.py:36\u001b[0m, in \u001b[0;36mLightningWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/timm/models/mobilenetv3.py:155\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 155\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[1;32m    156\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_head(x)\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/timm/models/mobilenetv3.py:136\u001b[0m, in \u001b[0;36mMobileNetV3.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_features\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    135\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_stem(x)\n\u001b[0;32m--> 136\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(x)\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_checkpointing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    138\u001b[0m         x \u001b[39m=\u001b[39m checkpoint_seq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, x, flatten\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/timm/layers/norm_act.py:85\u001b[0m, in \u001b[0;36mBatchNormAct2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     84\u001b[0m     \u001b[39m# cut & paste of torch.nn.BatchNorm2d.forward impl to avoid issues with torchscript and tracing\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     _assert(x\u001b[39m.\u001b[39;49mndim \u001b[39m==\u001b[39;49m \u001b[39m4\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexpected 4D input (got \u001b[39;49m\u001b[39m{\u001b[39;49;00mx\u001b[39m.\u001b[39;49mndim\u001b[39m}\u001b[39;49;00m\u001b[39mD input)\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     87\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/torch/__init__.py:1209\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(condition) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mTensor \u001b[39mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1208\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1209\u001b[0m \u001b[39massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "from lime import lime_image\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(np.array(batch[1]).transpose(1, 2, 0), \n",
    "                                         lambda x: trainer.predict(model, torch.from_numpy(x.transpose(0, 3, 1, 2))), # classification function\n",
    "                                         top_labels=2, \n",
    "                                         hide_color=0, \n",
    "                                         batch_size=1,\n",
    "                                         num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2624c7fa47ef49ff81f6ae6a0fdd6a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a192899ae9bf4f2ab7cdb9ac22d43588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mExplainer(\u001b[39mlambda\u001b[39;00m x: trainer\u001b[39m.\u001b[39mpredict(model, torch\u001b[39m.\u001b[39mTensor(x)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                            masker_blur,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                            output_names\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mNothing\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHighlight\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# feed only one image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# here we explain two images using 100 evaluations of the underlying model to estimate the SHAP values\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m shap_values \u001b[39m=\u001b[39m explainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     [ds[\u001b[39m0\u001b[39;49m]],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     max_evals\u001b[39m=\u001b[39;49mn_evals,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     outputs\u001b[39m=\u001b[39;49mshap\u001b[39m.\u001b[39;49mExplanation\u001b[39m.\u001b[39;49margsort\u001b[39m.\u001b[39;49mflip[:topk],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/londogard/git/hlondogard/lol_highlight_detection/explain_model.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/shap/explainers/_partition.py:128\u001b[0m, in \u001b[0;36mPartitionExplainer.__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_evals\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, fixed_context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    125\u001b[0m              outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    126\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[1;32m    129\u001b[0m         \u001b[39m*\u001b[39;49margs, max_evals\u001b[39m=\u001b[39;49mmax_evals, fixed_context\u001b[39m=\u001b[39;49mfixed_context, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    130\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/shap/explainers/_explainer.py:264\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     feature_names \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(args))]\n\u001b[1;32m    263\u001b[0m \u001b[39mfor\u001b[39;00m row_args \u001b[39min\u001b[39;00m show_progress(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39margs), num_rows, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m explainer\u001b[39m\u001b[39m\"\u001b[39m, silent):\n\u001b[0;32m--> 264\u001b[0m     row_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain_row(\n\u001b[1;32m    265\u001b[0m         \u001b[39m*\u001b[39;49mrow_args, max_evals\u001b[39m=\u001b[39;49mmax_evals, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds,\n\u001b[1;32m    266\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     values\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    269\u001b[0m     output_indices\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39moutput_indices\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/shap/explainers/_partition.py:176\u001b[0m, in \u001b[0;36mPartitionExplainer.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(out_shape)\n\u001b[1;32m    174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdvalues \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(out_shape)\n\u001b[0;32m--> 176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mowen(fm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_curr_base_value, f11, max_evals \u001b[39m-\u001b[39;49m \u001b[39m2\u001b[39;49m, outputs, fixed_context, batch_size, silent)\n\u001b[1;32m    178\u001b[0m \u001b[39m# if False:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m#     if self.multi_output:\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m#         return [self.dvalues[:,i] for i in range(self.dvalues.shape[1])], oinds\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# drop the interaction terms down onto self.values\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues[:] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdvalues\n",
      "File \u001b[0;32m~/micromamba/envs/highlights/lib/python3.11/site-packages/shap/explainers/_partition.py:230\u001b[0m, in \u001b[0;36mPartitionExplainer.owen\u001b[0;34m(self, fm, f00, f11, max_evals, output_indexes, fixed_context, batch_size, silent)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39m# make sure output_indexes is a list of indexes\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m output_indexes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[39m# assert self.multi_output, \"output_indexes is only valid for multi-output models!\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[39m# inds = output_indexes.apply(f11, 0)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39m# elif output_indexes.startswith(\"max(abs(\"):\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m#     output_indexes = np.argsort(np.abs(f11))[:out_len]\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     f00 \u001b[39m=\u001b[39m f00[output_indexes]\n\u001b[1;32m    231\u001b[0m     f11 \u001b[39m=\u001b[39m f11[output_indexes]\n\u001b[1;32m    233\u001b[0m q \u001b[39m=\u001b[39m queue\u001b[39m.\u001b[39mPriorityQueue()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "topk = 2\n",
    "batch_size = 50\n",
    "n_evals = 10000\n",
    "\n",
    "# define a masker that is used to mask out partitions of the input image.\n",
    "masker_blur = shap.maskers.Image(\"blur(128,128)\", ds[0].shape)\n",
    "\n",
    "# create an explainer with model and image masker\n",
    "explainer = shap.Explainer(lambda x: trainer.predict(model, torch.Tensor(x).unsqueeze(0)),\n",
    "                           masker_blur,\n",
    "                           output_names=[\"Nothing\", \"Highlight\"])\n",
    "\n",
    "# feed only one image\n",
    "# here we explain two images using 100 evaluations of the underlying model to estimate the SHAP values\n",
    "shap_values = explainer(\n",
    "    [ds[0]],\n",
    "    max_evals=n_evals,\n",
    "    batch_size=batch_size,\n",
    "    outputs=shap.Explanation.argsort.flip[:topk],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
