{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 3\n",
    "H, W, C = 512, 512, 3\n",
    "NUM_FRAMES = 30 # 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Sampler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomSubsequenceSampler(Sampler[List[int]]):\n",
    "    def __init__(self, num_samples: int, batch_size: int) -> None:\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.start_indices = np.random.permutation(len(self))\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        for index in self.start_indices:\n",
    "            index *= self.batch_size\n",
    "            yield torch.arange(index, min(index + self.batch_size, self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Define the duration of each chunk in seconds\n",
    "chunk_duration_s = 10\n",
    "chunk_duration_frames = 3 * chunk_duration_s\n",
    "\n",
    "# Define the path to the video frames directory\n",
    "frames_directory = 'frames/labeled_data/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# Create the ImageFolder dataset\n",
    "dataset = ImageFolder(root=frames_directory, transform=transform)\n",
    "\n",
    "# Calculate the total number of chunks\n",
    "total_frames = len(dataset)\n",
    "total_chunks = math.ceil(total_frames / chunk_duration_frames)\n",
    "\n",
    "# Create a list of chunk indices\n",
    "chunk_indices = [i for i in range(total_chunks)]\n",
    "\n",
    "# Split the chunk indices into training and validation sets\n",
    "train_size = int(0.8 * total_chunks)\n",
    "random.seed(42)\n",
    "random.shuffle(chunk_indices)\n",
    "train_chunk_indices = chunk_indices[:train_size]\n",
    "val_chunk_indices = chunk_indices[train_size:]\n",
    "\n",
    "# Create the training and validation subsets\n",
    "train_indices = [frame_idx for chunk_idx in train_chunk_indices for frame_idx in range(chunk_idx * chunk_duration_s, (chunk_idx + 1) * chunk_duration_s) if frame_idx < total_frames]\n",
    "val_indices = [frame_idx for chunk_idx in val_chunk_indices for frame_idx in range(chunk_idx * chunk_duration_s, (chunk_idx + 1) * chunk_duration_s) if frame_idx < total_frames]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# Create the data loaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/londogard/micromamba/envs/lol_highlights/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/londogard/micromamba/envs/lol_highlights/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class FineTuneResNet(pl.LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet34(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Instantiate the LightningModule and Trainer\n",
    "model = FineTuneResNet(num_classes=2)\n",
    "trainer = pl.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | resnet | ResNet | 21.3 M\n",
      "----------------------------------\n",
      "21.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.3 M    Total params\n",
      "85.143    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c1c45c49384d0283fc6d9dd4c6cc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~10m on one Epoch! :O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
