{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 3\n",
    "H, W, C = 512, 512, 3\n",
    "NUM_FRAMES = 30 # 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Define the duration of each chunk in seconds\n",
    "chunk_duration_s = 10\n",
    "chunk_duration_frames = 3 * chunk_duration_s\n",
    "\n",
    "# Define the path to the video frames directory\n",
    "frames_directory = 'frames/labeled_data/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# Create the ImageFolder dataset\n",
    "dataset = ImageFolder(root=frames_directory, transform=transform)\n",
    "\n",
    "# Calculate the total number of chunks\n",
    "total_frames = len(dataset)\n",
    "total_chunks = math.ceil(total_frames / chunk_duration_frames)\n",
    "\n",
    "# Create a list of chunk indices\n",
    "chunk_indices = [i for i in range(total_chunks)]\n",
    "\n",
    "# Split the chunk indices into training and validation sets\n",
    "train_size = int(0.8 * total_chunks)\n",
    "random.seed(42)\n",
    "random.shuffle(chunk_indices)\n",
    "train_chunk_indices = chunk_indices[:train_size]\n",
    "val_chunk_indices = chunk_indices[train_size:]\n",
    "\n",
    "# Create the training and validation subsets\n",
    "train_indices = [frame_idx for chunk_idx in train_chunk_indices for frame_idx in range(chunk_idx * chunk_duration_frames, (chunk_idx + 1) * chunk_duration_frames) if frame_idx < total_frames]\n",
    "val_indices = [frame_idx for chunk_idx in val_chunk_indices for frame_idx in range(chunk_idx * chunk_duration_frames, (chunk_idx + 1) * chunk_duration_frames) if frame_idx < total_frames]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# Create the data loaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
       " [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(train_indices)[:15], sorted(val_indices)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Optional\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as loggers\n",
    "import torchmetrics\n",
    "\n",
    "class FineTuneResNet(pl.LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(pretrained=True,)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        self.accuracy(logits, y)\n",
    "        self.log('train_acc_step', self.accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.val_accuracy(logits, y)\n",
    "        self.log('val_acc_step', self.val_accuracy)\n",
    "    \n",
    "    def on_validation_batch_end(self, outputs: STEP_OUTPUT | None, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n",
    "        self.log(\"valid_acc_epoch\", self.val_accuracy)\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Instantiate the LightningModule and Trainer\n",
    "model = FineTuneResNet(num_classes=2)\n",
    "trainer = pl.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type               | Params\n",
      "----------------------------------------------------\n",
      "0 | model        | ResNet             | 11.2 M\n",
      "1 | accuracy     | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy | MulticlassAccuracy | 0     \n",
      "----------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c75e32e6d6d4f2db89c838027063038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/londogard/micromamba/envs/lol_highlights/lib/python3.11/site-packages/torchmetrics/functional/classification/accuracy.py:65: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1680607563975/work/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  tp = tp.sum(dim=0 if multidim_average == \"global\" else 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aca28cfc6da49d4beaf097e46e49db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee9a4cd76d34915b12c76772cdd2c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"lightning.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~30m, as fast as Fast.AI really... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~10m on one Epoch! :O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
